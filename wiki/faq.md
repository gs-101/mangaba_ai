# ‚ùì Perguntas Frequentes (FAQ)

Esta se√ß√£o cont√©m respostas para as perguntas mais comuns sobre o Mangaba AI. Se sua d√∫vida n√£o estiver aqui, consulte as [Issues no GitHub](https://github.com/Mangaba-ai/mangaba_ai/issues) ou abra uma nova pergunta.

## üìã √çndice

1. [Instala√ß√£o e Configura√ß√£o](#-instala√ß√£o-e-configura√ß√£o)
2. [Primeiros Passos](#-primeiros-passos)
3. [Protocolos A2A e MCP](#-protocolos-a2a-e-mcp)
4. [Desenvolvimento e API](#-desenvolvimento-e-api)
5. [Performance e Otimiza√ß√£o](#-performance-e-otimiza√ß√£o)
6. [Troubleshooting](#-troubleshooting)
7. [Contribui√ß√£o e Comunidade](#-contribui√ß√£o-e-comunidade)

---

## üîß Instala√ß√£o e Configura√ß√£o

### ‚ùì Quais s√£o os requisitos m√≠nimos de sistema?

**R:** Os requisitos m√≠nimos s√£o:
- **Python 3.8+** (recomendado 3.10+)
- **4GB RAM** (8GB+ recomendado)
- **2GB espa√ßo em disco** 
- **Conex√£o com internet** (para APIs de IA)

```bash
# Verificar vers√£o do Python
python --version

# Deve retornar Python 3.8.0 ou superior
```

### ‚ùì Como obter uma API key do Google Gemini?

**R:** Siga estes passos:

1. **Acesse**: [Google AI Studio](https://aistudio.google.com/)
2. **Fa√ßa login** com sua conta Google
3. **Clique em "Get API Key"**
4. **Selecione ou crie um projeto**
5. **Copie a chave gerada**
6. **Configure no .env**:
   ```bash
   GEMINI_API_KEY=AIzaSyD...sua_chave_aqui
   ```

### ‚ùì √â poss√≠vel usar outros modelos de IA al√©m do Gemini?

**R:** Sim! O Mangaba AI suporta m√∫ltiplos provedores:

```python
# Configura√ß√£o para OpenAI
agent = ai.create_agent(
    name="gpt_agent",
    model_provider="openai",
    model_name="gpt-4"
)

# Configura√ß√£o para modelos locais (Ollama)
agent = ai.create_agent(
    name="local_agent", 
    model_provider="ollama",
    model_name="llama2"
)
```

### ‚ùì Como configurar o projeto em ambiente Windows?

**R:** Recomendamos usar WSL2:

```bash
# 1. Instale WSL2
wsl --install

# 2. No WSL2, siga instala√ß√£o normal Linux
git clone https://github.com/Mangaba-ai/mangaba_ai.git
cd mangaba_ai

# 3. Configure ambiente virtual
python -m venv .venv
source .venv/bin/activate

# 4. Instale depend√™ncias
pip install -r requirements.txt
```

Alternativamente, use PowerShell:
```powershell
# Use .venv\Scripts\activate.ps1 em vez de source
.venv\Scripts\activate.ps1
```

## üöÄ Primeiros Passos

### ‚ùì Como criar meu primeiro agente?

**R:** Exemplo b√°sico:

```python
from mangaba_ai import MangabaAI
import asyncio

async def primeiro_agente():
    # Inicializar framework
    ai = MangabaAI()
    
    # Criar agente
    agente = ai.create_agent(
        name="assistente",
        role="Assistente Pessoal",
        goal="Ajudar com tarefas gerais"
    )
    
    # Usar o agente
    resposta = await agente.chat("Ol√°! Como voc√™ pode me ajudar?")
    print(resposta)

# Executar
asyncio.run(primeiro_agente())
```

### ‚ùì Como fazer agentes conversarem entre si?

**R:** Use o protocolo A2A:

```python
# Agente A envia mensagem para Agente B
await agente_a.send_message(
    receiver="agente_b",
    content="Preciso de an√°lise dos dados X",
    priority=2
)

# Agente B recebe e processa
mensagens = await agente_b.receive_messages()
for msg in mensagens:
    resultado = await agente_b.process(msg.content)
    # Enviar resposta de volta
    await agente_b.send_message(
        receiver=msg.sender,
        content=resultado
    )
```

### ‚ùì Como manter contexto entre conversas?

**R:** O protocolo MCP gerencia isso automaticamente:

```python
# Primeira conversa
resposta1 = await agente.chat("Meu nome √© Jo√£o e trabalho com vendas")

# Segunda conversa (contexto mantido)
resposta2 = await agente.chat("Qual √© meu nome?")
# Resposta: "Seu nome √© Jo√£o"

# Para sess√µes espec√≠ficas
with agente.mcp_protocol.session("usuario_123"):
    await agente.chat("Conversa isolada por usu√°rio")
```

## üåê Protocolos A2A e MCP

### ‚ùì Qual a diferen√ßa entre protocolos A2A e MCP?

**R:** 

**A2A (Agent-to-Agent)**:
- **Finalidade**: Comunica√ß√£o entre agentes
- **Uso**: Coordena√ß√£o, troca de mensagens, workflows
- **Exemplo**: Agente pesquisador envia dados para agente analista

**MCP (Model Context Protocol)**:
- **Finalidade**: Gerenciamento de contexto e mem√≥ria
- **Uso**: Manter hist√≥rico, sess√µes, contexto de conversas
- **Exemplo**: Lembrar informa√ß√µes do usu√°rio entre sess√µes

### ‚ùì Como configurar TTL (Time To Live) para mensagens A2A?

**R:** Configure TTL na configura√ß√£o ou por mensagem:

```python
# Configura√ß√£o global
agent.a2a_protocol.set_default_ttl(seconds=3600)  # 1 hora

# Por mensagem espec√≠fica
await agent.send_message(
    receiver="outro_agente",
    content="Mensagem urgente",
    ttl=300  # 5 minutos
)
```

### ‚ùì Como limpar contexto MCP antigo?

**R:** V√°rias op√ß√µes de limpeza:

```python
# Limpeza autom√°tica por tempo
agent.mcp_protocol.set_auto_cleanup(
    max_age_hours=24,
    cleanup_interval_minutes=60
)

# Limpeza manual seletiva
agent.mcp_protocol.clear_context(
    context_type="chat_history",
    older_than_hours=12
)

# Limpeza completa (cuidado!)
agent.mcp_protocol.clear_all_context()
```

### ‚ùì Como implementar broadcast para m√∫ltiplos agentes?

**R:** Use o m√©todo broadcast:

```python
# Lista de agentes receptores
receptores = ["agente_1", "agente_2", "agente_3"]

# Envio em broadcast
await coordenador.broadcast_message(
    receivers=receptores,
    content={"task": "analyze_data", "deadline": "2024-01-15"},
    message_type="task_assignment",
    priority=2
)

# Coleta de respostas
respostas = []
for agente_nome in receptores:
    agente = ai.get_agent(agente_nome)
    mensagens = await agente.receive_messages()
    if mensagens:
        resposta = await agente.process_task(mensagens[0])
        respostas.append(resposta)
```

## üíª Desenvolvimento e API

### ‚ùì Como estender o framework com novas ferramentas?

**R:** Crie uma classe que herda de `BaseTool`:

```python
from mangaba_ai.core.tools import BaseTool

class MinhaFerramentaCustomizada(BaseTool):
    name = "minha_ferramenta"
    description = "Descri√ß√£o da ferramenta"
    
    async def execute(self, params: dict) -> dict:
        # Implementar l√≥gica da ferramenta
        resultado = self.processar(params)
        return {"resultado": resultado}
    
    def processar(self, params: dict):
        # L√≥gica espec√≠fica
        return "Resultado processado"

# Registrar ferramenta
agent.add_tool(MinhaFerramentaCustomizada())
```

### ‚ùì Como criar agentes especializados?

**R:** Exemplo de agente especializado:

```python
class AgentePesquisador(MangabaAgent):
    def __init__(self, **kwargs):
        super().__init__(
            role="Researcher",
            goal="Coletar e analisar informa√ß√µes",
            **kwargs
        )
        
        # Adicionar ferramentas espec√≠ficas
        self.add_tool(GoogleSearchTool())
        self.add_tool(WebScrapingTool())
        
    async def pesquisar_topico(self, topico: str) -> dict:
        # Implementar l√≥gica de pesquisa
        resultados = await self.tools["google_search"].execute({
            "query": topico,
            "num_results": 10
        })
        
        analise = await self.analisar_resultados(resultados)
        return analise
```

### ‚ùì Como integrar com APIs externas?

**R:** Crie adaptadores para APIs:

```python
class SlackIntegration:
    def __init__(self, token: str):
        self.client = SlackClient(token)
        
    async def enviar_para_slack(self, canal: str, mensagem: str):
        return await self.client.chat_postMessage(
            channel=canal,
            text=mensagem
        )

# Uso no agente
class AgenteSlack(MangabaAgent):
    def __init__(self, slack_token: str, **kwargs):
        super().__init__(**kwargs)
        self.slack = SlackIntegration(slack_token)
        
    async def notificar_slack(self, mensagem: str):
        await self.slack.enviar_para_slack("#geral", mensagem)
```

## ‚ö° Performance e Otimiza√ß√£o

### ‚ùì Como otimizar performance com muitos agentes?

**R:** Implementar estrat√©gias de otimiza√ß√£o:

```python
# 1. Pool de agentes
class AgentPool:
    def __init__(self, size: int = 10):
        self.agents = [self.create_agent() for _ in range(size)]
        self.queue = asyncio.Queue()
    
    async def get_agent(self):
        return await self.queue.get()
    
    async def return_agent(self, agent):
        await self.queue.put(agent)

# 2. Processamento batch
async def processar_batch(mensagens: List[str], batch_size: int = 5):
    for i in range(0, len(mensagens), batch_size):
        batch = mensagens[i:i + batch_size]
        tasks = [agent.process(msg) for msg in batch]
        await asyncio.gather(*tasks)

# 3. Cache de respostas
@lru_cache(maxsize=1000)
async def cached_ai_call(prompt: str) -> str:
    return await ai_model.generate(prompt)
```

### ‚ùì Como monitorar uso de recursos?

**R:** Implemente monitoramento:

```python
import psutil
import time

class ResourceMonitor:
    def __init__(self):
        self.start_time = time.time()
        self.initial_memory = psutil.virtual_memory().used
    
    def get_stats(self) -> dict:
        current_memory = psutil.virtual_memory().used
        cpu_percent = psutil.cpu_percent()
        
        return {
            "uptime": time.time() - self.start_time,
            "memory_usage_mb": (current_memory - self.initial_memory) / 1024 / 1024,
            "cpu_percent": cpu_percent,
            "active_agents": len(ai.active_agents)
        }

# Uso
monitor = ResourceMonitor()
stats = monitor.get_stats()
print(f"Mem√≥ria: {stats['memory_usage_mb']:.1f}MB")
```

### ‚ùì Como configurar rate limiting?

**R:** Configure limites de requisi√ß√µes:

```python
# Rate limiting por agente
agent.set_rate_limit(
    requests_per_minute=30,
    burst_allowance=5
)

# Rate limiting global
ai.set_global_rate_limit(
    total_requests_per_minute=100,
    per_agent_limit=20
)

# Rate limiting customizado
class CustomRateLimiter:
    def __init__(self, max_requests: int, window_seconds: int):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests = []
    
    async def check_limit(self) -> bool:
        now = time.time()
        # Remove requisi√ß√µes antigas
        self.requests = [req for req in self.requests 
                        if now - req < self.window_seconds]
        
        if len(self.requests) >= self.max_requests:
            return False
            
        self.requests.append(now)
        return True
```

## üîß Troubleshooting

### ‚ùì Como debugar problemas de comunica√ß√£o A2A?

**R:** Use ferramentas de debug:

```python
# 1. Habilitar logs detalhados
import logging
logging.basicConfig(level=logging.DEBUG)

# 2. Monitor de mensagens
class A2AMonitor:
    def __init__(self, agent):
        self.agent = agent
        
    async def monitor_messages(self):
        while True:
            stats = await self.agent.a2a_protocol.get_stats()
            print(f"üìä Mensagens pendentes: {stats['pending']}")
            print(f"üìà Total enviadas: {stats['sent']}")
            print(f"üìâ Total recebidas: {stats['received']}")
            await asyncio.sleep(10)

# 3. Verificar conectividade
async def test_connectivity():
    try:
        await agent_a.send_message(
            receiver="agent_b",
            content="ping",
            timeout=5
        )
        print("‚úÖ Comunica√ß√£o A2A funcionando")
    except TimeoutError:
        print("‚ùå Timeout na comunica√ß√£o A2A")
    except Exception as e:
        print(f"‚ùå Erro: {e}")
```

### ‚ùì Como resolver problemas de mem√≥ria MCP?

**R:** Estrat√©gias de diagn√≥stico e solu√ß√£o:

```python
# 1. Diagn√≥stico de contexto
def diagnosticar_mcp(agent):
    stats = agent.mcp_protocol.get_context_stats()
    print("üìä Estat√≠sticas MCP:")
    for tipo, dados in stats.items():
        print(f"  {tipo}: {len(dados)} itens, {dados.memory_usage_mb:.1f}MB")
    
    # Verificar contextos expirados
    expired = agent.mcp_protocol.get_expired_contexts()
    print(f"‚è∞ Contextos expirados: {len(expired)}")

# 2. Limpeza inteligente
async def limpeza_inteligente(agent):
    # Remover contextos menos relevantes
    await agent.mcp_protocol.cleanup_by_relevance(
        keep_top_percent=80,
        max_age_hours=24
    )
    
    # Compactar contextos relacionados
    await agent.mcp_protocol.compact_related_contexts()

# 3. Configura√ß√£o otimizada
agent.mcp_protocol.configure(
    max_context_size_mb=100,
    auto_cleanup_interval=1800,  # 30 minutos
    compression_enabled=True
)
```

### ‚ùì Como resolver erros de API key?

**R:** Checklist de diagn√≥stico:

```python
# 1. Verificar configura√ß√£o
import os
from dotenv import load_dotenv

load_dotenv()
gemini_key = os.getenv("GEMINI_API_KEY")

if not gemini_key:
    print("‚ùå GEMINI_API_KEY n√£o configurada")
elif len(gemini_key) < 20:
    print("‚ùå GEMINI_API_KEY parece incompleta")
else:
    print("‚úÖ GEMINI_API_KEY configurada")

# 2. Testar conectividade
async def test_api_key():
    try:
        # Teste simples
        agent = ai.create_agent("test", "Test", "Test")
        response = await agent.chat("Hello")
        print("‚úÖ API key funcionando")
        return True
    except AuthenticationError:
        print("‚ùå API key inv√°lida")
        return False
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        return False

# 3. Verificar limites
async def check_api_limits():
    # Implementar verifica√ß√£o de cota/limites
    usage = await ai.get_api_usage()
    print(f"üìä Uso atual: {usage.requests_today}/{usage.daily_limit}")
```

## ü§ù Contribui√ß√£o e Comunidade

### ‚ùì Como contribuir com o projeto?

**R:** Siga estes passos:

1. **Fork o reposit√≥rio**
2. **Crie uma branch para sua feature**:
   ```bash
   git checkout -b feature/minha-feature
   ```
3. **Fa√ßa suas altera√ß√µes e testes**
4. **Commit com mensagem clara**:
   ```bash
   git commit -m "feat: adiciona funcionalidade X"
   ```
5. **Abra um Pull Request**

Veja o [Guia de Contribui√ß√£o](contribuicao.md) completo.

### ‚ùì Como reportar bugs?

**R:** Use o template de bug report no GitHub:

1. **Descreva o problema** claramente
2. **Inclua passos para reproduzir**
3. **Adicione informa√ß√µes do ambiente**:
   ```bash
   python --version
   pip list | grep mangaba
   uname -a
   ```
4. **Anexe logs relevantes**
5. **Mencione comportamento esperado vs atual**

### ‚ùì Onde buscar ajuda?

**R:** Recursos dispon√≠veis:

- üìñ **[Documenta√ß√£o completa](README.md)** - In√≠cio aqui
- üêõ **[Issues no GitHub](https://github.com/Mangaba-ai/mangaba_ai/issues)** - Bugs e problemas
- üí¨ **[Discussions](https://github.com/Mangaba-ai/mangaba_ai/discussions)** - Perguntas e ideias
- üìß **Email**: suporte@mangaba.ai
- üí¨ **Discord**: [Link da comunidade]

### ‚ùì Como sugerir novas funcionalidades?

**R:** Use GitHub Discussions ou Issues:

1. **Verifique se j√° n√£o existe** solicita√ß√£o similar
2. **Descreva a funcionalidade** e motiva√ß√£o
3. **Explique o caso de uso** com exemplos
4. **Considere implementa√ß√£o** se poss√≠vel
5. **Use labels apropriadas**: `enhancement`, `feature-request`

---

## üîó Links √öteis

- üìö [Documenta√ß√£o Principal](README.md)
- üöÄ [Guia de Instala√ß√£o](instalacao-configuracao.md) 
- üåê [Exemplos de Protocolos](exemplos-protocolos.md)
- ü§ù [Como Contribuir](contribuicao.md)
- üìñ [Vis√£o Geral](visao-geral.md)

---

> ‚ùì **N√£o encontrou sua resposta?** Abra uma [Issue](https://github.com/Mangaba-ai/mangaba_ai/issues/new) ou [Discussion](https://github.com/Mangaba-ai/mangaba_ai/discussions/new) no GitHub!